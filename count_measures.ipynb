{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from syn_compl import (get_parsed_text, count_tokens, \n",
    "av_depth_for_one_text, min_depth_for_one_text, \n",
    "max_depth_for_one_text, count_acl, count_acl_relcl,\n",
    "count_advcl, count_sent, count_clauses, count_tunits,\n",
    "count_complex_tunit, count_coord, count_np, count_vp,\n",
    "lemma_sim_min, pos_sim_min, tokens_befor_root, mean_len_sent)\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_hole_file(name_file):\n",
    "    f = open(name_file, 'r')\n",
    "    f_r = f.read()\n",
    "    return f_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "esse_gr_lst = read_hole_file('/Users/irene/Desktop/Курсовая/grade.txt').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string2 = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for esse in esse_gr_lst:\n",
    "    name = re.search('(.+?)\\t', esse).group(1)\n",
    "    #print(name)\n",
    "    path = '/Users/irene/Desktop/Курсовая/esseys/' + name + '.txt'\n",
    "    parsed_text = get_parsed_text('english-partut-ud-2.0-170801.udpipe', path)\n",
    "    # +++\n",
    "    num_tokens = count_tokens(parsed_text)# number of tokens\n",
    "    # +++\n",
    "    av_depth = av_depth_for_one_text(parsed_text)# average depth of tree\n",
    "    # +++\n",
    "    min_depth = min_depth_for_one_text(parsed_text)# minimal depth of tree\n",
    "    # +++\n",
    "    max_depth = max_depth_for_one_text(parsed_text)# maximal depth of tree\n",
    "    # +++\n",
    "    num_acl = count_acl(parsed_text)# number of acls\n",
    "    num_acl_relcl = count_acl_relcl(parsed_text)# number of acl:relcls\n",
    "    num_advcl = count_advcl(parsed_text)# number of advcls\n",
    "    num_sents = count_sent(parsed_text)# number of sentances\n",
    "    num_cl = count_clauses(parsed_text)# number of clauses\n",
    "    num_tu = count_tunits(parsed_text)# number of T-units\n",
    "    # +++\n",
    "    num_ctu = count_complex_tunit(parsed_text)# number of complex T-units\n",
    "    # +++\n",
    "    num_coord = count_coord(parsed_text)# number of coordinational phrases\n",
    "    num_np = count_np(parsed_text)# number of nps: possesive constructions, prepositional phrases, \n",
    "                                    #adj + nouns, gerund + inf, part + nouns \n",
    "    num_vp = count_vp(parsed_text)# number of vps\n",
    "    # +++\n",
    "    mean_l_sim = lemma_sim_min(parsed_text)# mean L. distance (lemmas)\n",
    "    # +++\n",
    "    mean_p_sim = pos_sim_min(parsed_text)# mean L. distance (pos)\n",
    "    # +++\n",
    "    mean_tokens_root = tokens_befor_root(parsed_text)# number of tokens before root\n",
    "    # +++\n",
    "#--COMPLEX MEASURES--#\n",
    "    mean_length_s = mean_len_sent(parsed_text)# mean length of sentances\n",
    "    mean_length_c = num_tokens/num_cl\n",
    "    c_s = num_cl/num_sents\n",
    "    c_t = num_cl/num_tu\n",
    "    acl_t = num_acl/num_tu\n",
    "    acl_relcl_t = num_acl_relcl/num_tu\n",
    "    advcl_t = num_advcl/num_tu\n",
    "    acl_cl = num_acl/num_cl\n",
    "    acl_relcl_cl = num_acl_relcl/num_cl\n",
    "    advcl_cl = num_advcl/num_cl\n",
    "    coord_cl = num_coord/num_cl\n",
    "    t_s = num_tu/num_sents\n",
    "    poss_s = num_np[0]/num_sents\n",
    "    prep_s = num_np[1]/num_sents\n",
    "    adj_n_s = num_np[2]/num_sents\n",
    "    ger_inf_s = num_np[3]/num_sents\n",
    "    part_n_s = num_np[4]/num_sents\n",
    "    vp_s = num_vp/num_sents\n",
    "\n",
    "    simple_measures = [num_tokens, num_acl, num_acl_relcl, num_advcl, num_sents, num_cl, num_tu,\n",
    "                num_coord, num_np[0], num_np[1], num_np[2], num_np[3], num_np[4], num_vp,\n",
    "                       av_depth, min_depth, max_depth, num_ctu, mean_l_sim, mean_length_s, mean_length_c,\n",
    "               mean_p_sim, mean_tokens_root]\n",
    "    \n",
    "    measures = [num_acl, num_acl_relcl, num_advcl, num_tokens, num_sents, num_cl, num_tu,\n",
    "                num_coord, sum(num_np), num_vp, \n",
    "                av_depth, min_depth, max_depth, num_ctu, mean_l_sim,\n",
    "               mean_p_sim, mean_tokens_root, mean_length_s, mean_length_c,\n",
    "               c_s, c_t, acl_t, acl_relcl_t, advcl_t, acl_cl, acl_relcl_cl, advcl_cl,\n",
    "               coord_cl, t_s, poss_s, prep_s, adj_n_s, ger_inf_s, part_n_s, vp_s]\n",
    "    str_one_esse = ''\n",
    "    for measure in measures:\n",
    "        str_one_esse = str_one_esse + str(round(measure, 3)) + '\\t'\n",
    "    str_one_esse2 = ''\n",
    "    for m in simple_measures:\n",
    "        str_one_esse2 = str_one_esse2 + str(round(m, 4)) + '\\t'\n",
    "    #print(str_one_esse)\n",
    "    string = string + esse + '\\t' + str_one_esse + '\\n'\n",
    "    string2 = string2 + esse + '\\t' + str_one_esse2 + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*средняя длина предложения (MLS)\n",
    "\n",
    "*средняя длина клаузы (MLC)\n",
    "\n",
    "*количество клауз на предложение (C/S)\n",
    "\n",
    "*количество клауз (C/T)\n",
    "\n",
    "*сложных Т-юнитов (CT/T)\n",
    "\n",
    "*зависимых клауз (DC/T) на Т-юнит (по отдельности)\n",
    "\n",
    "*количество зависимых клауз на клаузу (DC/C) (по отдельности)\n",
    "\n",
    "*количество сочинительных групп на клаузу (CP/C)\n",
    "\n",
    "*количество Т-юнитов на предложение (T/S)\n",
    "\n",
    "*количество сложных именных групп на клаузу (CN/C) (по отдельности)\n",
    "\n",
    "*количество глагольных групп на Т-юнит (VP/T)\n",
    "\n",
    "*количество токенов, стоящих перед рутом в предложении (SYNLE)\n",
    "\n",
    "*среднее расстояние Левенштейна между частями речи (SYNMEDpos)\n",
    "\n",
    "*среднее расстояние Левенштейна между леммами (SYNMEDlem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('/Users/irene/Desktop/Курсовая/res_measures.csv', 'w')\n",
    "f.write(string)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = open('/Users/irene/Desktop/Курсовая/res_measures_simple.csv', 'w')\n",
    "c.write(string2)\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
